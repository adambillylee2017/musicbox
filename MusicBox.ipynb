{"cells":[{"cell_type":"code","source":["dbutils.fs.rm(\"/FileStore/tables/play\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%fs \nls /FileStore/tables/play"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType([\n  StructField(\"uid\", StringType(), True),\n  StructField(\"song_id\", StringType(), True),\n  StructField(\"song_name\", StringType(), True),\n  StructField(\"singer\", StringType(), True),\n  StructField(\"play_time\", StringType(), True),\n  StructField(\"song_length\", StringType(), True),\n  StructField(\"date\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["play = spark.createDataFrame([],schema=schema)\nfor path, name, _ in dbutils.fs.ls(\"/FileStore/tables/play\"):\n  df = spark.read.csv(path=path, schema=schema, header=True)\n  play = play.unionAll(df)\nplay.createOrReplaceTempView(\"play\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\nselect *\nfrom play\nlimit 10"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql\nselect \n  cast(trim(uid) as bigint) as uid, \n  cast(trim(song_id) as bigint) as song_id, \n  trim(song_name) as song_name,\n  trim(singer) as singer,\n  cast(trim(play_time) as bigint) as play_time, \n  cast(trim(song_length) as bigint) as song_length, \n  TO_DATE(cast(UNIX_TIMESTAMP(date, 'yyyy-MM-dd') as TIMESTAMP)) as date \nfrom play\nwhere \n  uid is not null and\n  song_id is not null and\n  play_time is not null and\n  song_length is not null and\n  date is not null\nlimit 10;"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["play_filtered = sqlContext.sql(\"\"\"\n  select \n    cast(trim(uid) as bigint) as uid, \n    cast(trim(song_id) as bigint) as song_id, \n    trim(song_name) as song_name,\n    trim(singer) as singer,\n    cast(trim(play_time) as bigint) as play_time, \n    cast(trim(song_length) as bigint) as song_length, \n    TO_DATE(cast(UNIX_TIMESTAMP(date, 'yyyy-MM-dd') as TIMESTAMP)) as date \n  from play\n  where \n    uid is not null and\n    song_id is not null and\n    play_time is not null and\n    song_length is not null and\n    date is not null\n\"\"\")\n# play_filtered.write.saveAsTable(\"play_filtered\")\nplay_filtered.createOrReplaceTempView(\"play_filtered\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\nselect\n  count(*)\nfrom play_filtered"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql\n-- find out how many play records has song_length = 0\nselect\n  count(*)\nfrom play_filtered\nwhere \n  song_length = 0;"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sql\n-- find out how many songs there are\nselect\n  count(distinct(song_id))\nfrom play_filtered"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql\n-- find out how may data a affected by inconsistent song_id\nwith bad_song_id as (\n  select\n    song_id,\n    count(distinct(song_name)) as num_song_name\n  from play_filtered\n  group by song_id\n  having num_song_name > 1\n)\n\nselect \n  count(*)\nfrom play_filtered p\njoin bad_song_id b on p.song_id = b.song_id"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql\n-- find out how many users there are\nselect\n  count(distinct(uid))\nfrom play_filtered"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql\n-- perentile of song length\nselect\n  percentile(song_length, 0.999) as song_length_p999,\n  percentile(song_length, 0.99) as song_length_p99,\n  percentile(song_length, 0.95) as song_length_p95,\n  percentile(song_length, 0.90) as song_length_p90,\n  percentile(song_length, 0.50) as song_length_p50\nfrom play_filtered"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sql\n-- any one listen to songs for more than 24 hours in a day?\nselect count(distinct(uid)) as invalid_uid\nfrom (\n  select\n    uid,\n    date,\n    sum(play_time) as total_play_time\n  from play_filtered\n  group by uid, date\n  having sum(play_time) > 24 * 60 * 60\n) p"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql\n-- # how many records does per uid have\nselect \n  percentile(cnt, 0.999) as cnt_p999,\n  percentile(cnt, 0.99) as cnt_p99,\n  percentile(cnt, 0.95) as cnt_p95,\n  percentile(cnt, 0.90) as cnt_p90,\n  percentile(cnt, 0.50) as cnt_p50\nfrom (\n  select\n    uid,\n    date,\n    count(*) as cnt\n  from play_filtered\n  group by uid, date\n) p"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql\n-- for each uid in each date, how many times that a song id will appear\nselect \n  percentile(cnt, 0.999) as cnt_p999,\n  percentile(cnt, 0.99) as cnt_p99,\n  percentile(cnt, 0.95) as cnt_p95,\n  percentile(cnt, 0.90) as cnt_p90,\n  percentile(cnt, 0.50) as cnt_p50\nfrom (\n  select\n    uid,\n    song_id,\n    date,\n    count(*) as cnt\n  from play_filtered\n  group by uid, song_id, date\n) p"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\n-- what are songs with multiple song length?\nselect \n  song_id, \n  count(distinct(song_length)) as cnt\nfrom play_filtered\ngroup by song_id\nhaving cnt > 1\norder by cnt desc"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sql\n-- how many song have different song length\nselect\n  count(song_id)\nfrom (\n  select \n    song_id\n  from play_filtered\n  group by song_id\n  having count(distinct(song_length)) > 1\n) p"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["new_song_id = sqlContext.sql(\n\"\"\"\nselect\n  song_name,\n  singer,\n  row_number() over (order by song_name, singer) as sid\nfrom\n  play_filtered\ngroup by song_name, singer\n\"\"\"\n)\nnew_song_id.createOrReplaceTempView(\"new_song_id\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["play_reindexed = sqlContext.sql(\n\"\"\"\nselect\n  uid,\n  sid,\n  n.song_name,\n  n.singer,\n  play_time,\n  song_length,\n  date\nfrom play_filtered p\njoin new_song_id n\non p.song_name=n.song_name\nand p.singer=n.singer\n\"\"\"\n)\nplay_reindexed.createOrReplaceTempView(\"play_reindexed\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# repartition play_reindexed\n# play_reindexed = play_reindexed.repartition(\"date\")\n# play_reindexed.write.saveAsTable(\"play_reindexed\")\nplay_reindexed.cache()\nsqlContext.cacheTable(\"play_reindexed\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%sql\nselect\n  sid,\n  song_name,\n  singer,\n  count(distinct(song_length)) as count\nfrom\n  play_reindexed\ngroup by sid, song_name, singer\nhaving count > 1\norder by count desc"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%sql\nwith song_length_rank as\n(\n  select\n      sid,\n      song_length,\n      rank() over (partition by sid order by count(*) desc) as rank\n    from\n      play_reindexed\n    group by sid, song_length\n),\nsid_song_length as \n(\n  select\n    sid,\n    song_length\n  from\n    song_length_rank\n  where rank=1\n)\nselect \n  uid,\n  p.sid,\n  song_name,\n  singer,\n  play_time,\n  l.song_length,\n  date\nfrom play_reindexed p\njoin sid_song_length l on p.sid=l.sid\nwhere l.song_length > 0"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# for each sid that has multiple song_length, reassign song length to be the one with most count\n# and drop records with 0 song length\nplay_reindexed = sqlContext.sql(\n\"\"\"\nwith song_length_rank as\n(\n  select\n      sid,\n      song_length,\n      rank() over (partition by sid order by count(*) desc) as rank\n    from\n      play_reindexed\n    group by sid, song_length\n),\nsid_song_length as \n(\n  select\n    sid,\n    song_length\n  from\n    song_length_rank\n  where rank=1\n)\nselect \n  uid,\n  p.sid,\n  song_name,\n  singer,\n  play_time,\n  l.song_length,\n  date\nfrom play_reindexed p\njoin sid_song_length l on p.sid=l.sid\nwhere l.song_length > 0\n\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%sql\n-- in this data set, how many songs does most people plays?\nselect\n  percentile(count, 0.95) as P99,\n  percentile(count, 0.99) as P99,\n  percentile(count, 0.999) as P999\nfrom (\n  select \n    count(*) as count\n  from\n    play_reindexed\n  group by\n    uid, sid\n) p"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql\nselect\n    uid,\n    sid,\n    rank() over (partition by uid,sid order by count(*) desc) as rank\n  from\n    play_reindexed\n  group by uid"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# drop bot user bases on play count\nplay_reindexed = sqlContext.sql(\n\"\"\"\nwith song_length_rank as\n(\n  select\n      sid,\n      song_length,\n      rank() over (partition by sid order by count(*) desc) as rank\n    from\n      play_reindexed\n    group by sid, song_length\n),\nsid_song_length as \n(\n  select\n    sid,\n    song_length\n  from\n    song_length_rank\n  where rank=1\n)\nselect \n  uid,\n  p.sid,\n  song_name,\n  singer,\n  play_time,\n  l.song_length,\n  date\nfrom play_reindexed p\njoin sid_song_length l on p.sid=l.sid\nwhere l.song_length > 0\n\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["\"\"\"\nmake utility matrix from play_indexed\n\"\"\"\ndef createUtility(df, play_threashold):\n  df.createOrReplaceTempView(\"raw\");\n  utility = spark.sql(\"\"\"\n    select\n      uid,\n      sid,\n      date,\n      count(*) as count\n    from raw\n    where play_time * 1.0 / song_length > 0.6 and play_time is not null\n    group by uid, sid, date\n  \"\"\")\n  \n  return utility\n\nutil = createUtility(play_reindexed, 0.6)\nutil.createOrReplaceTempView(\"util\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["util.show(1)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql\nselect\n  date,\n  count(*)\nfrom util\ngroup by date\norder by date"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["test = sqlContext.sql(\n\"\"\"\nselect\n  *\nfrom util\nwhere date >= \"2017-05-06\"\n\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["training = sqlContext.sql(\n\"\"\"\nselect\n  *\nfrom util\nwhere date >= \"2017-04-06\" and\n  date < \"2017-05-06\"\n\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nals = ALS(maxIter=5, regParam=0.01, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"count\",\n          coldStartStrategy=\"drop\", nonnegative=True, implicitPrefs=True)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["from pyspark.ml.evaluation import Evaluator\n\nclass TestEvaluator(Evaluator):    \n  def _evaluate(self, dataset):\n    dataset.createOrReplaceTempView(\"dataset\")\n    result = spark.sql(\n    \"\"\"\n    with r as (\n      select\n        uid,\n        sid,\n        count,\n        (rank() over (partition by uid order by prediction desc ) -1) / (count(sid) over (partition by uid) - 1) as score\n      from dataset\n    )\n\n    select\n      sum(score) / sum(count)\n    from r\n    \"\"\"                 \n    ).collect()[0][0]\n    \n    return float(result)\n\nevaluator = TestEvaluator()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.ml.tuning import TrainValidationSplit, TrainValidationSplitModel\nimport numpy as np\nclass CustomTrainValidationSplit(TrainValidationSplit):\n    def __init__(self, validation, training, estimator=None, estimatorParamMaps=None, evaluator=None):\n        super(CustomTrainValidationSplit, self).__init__()\n        self.estimator = estimator\n        self.evaluator = evaluator\n        self.estimatorParamMaps = estimatorParamMaps\n        self.validation = validation\n        self.training = training\n\n    def _fit(self, dataset):\n        est = self.estimator\n        epm = self.estimatorParamMaps\n        numModels = len(epm)\n        eva = self.evaluator\n        metrics = [0.0] * numModels\n        validation = self.validation\n        train = self.training\n        for j in range(numModels):\n            model = est.fit(train, epm[j])\n            metric = eva.evaluate(model.transform(validation, epm[j]))\n            metrics[j] += metric\n        if eva.isLargerBetter():\n            bestIndex = np.argmax(metrics)\n        else:\n            bestIndex = np.argmin(metrics)\n        bestModel = est.fit(dataset, epm[bestIndex])\n        return self._copyValues(TrainValidationSplitModel(bestModel, metrics))\n      \n    def _copyValues(self, to, extra=None):\n        \"\"\"\n        Copies param values from this instance to another instance for\n        params shared by them.\n\n        :param to: the target instance\n        :param extra: extra params to be copied\n        :return: the target instance with param values copied\n        \"\"\"\n        paramMap = self._paramMap.copy()\n        if extra is not None:\n            paramMap.update(extra)\n        for param in self.params:\n            # copy default params\n            if param in self._defaultParamMap and to.hasParam(param.name):\n                to._defaultParamMap[to.getParam(param.name)] = self._defaultParamMap[param]\n            # copy explicitly set params\n            if param in paramMap and to.hasParam(param.name):\n                to._set(**{param.name: paramMap[param]})\n        return to"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.recommendation import ALS\nals = ALS(regParam=0.01, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"count\",\n          coldStartStrategy=\"drop\", nonnegative=True, implicitPrefs=True)\nparamGrid = ParamGridBuilder().addGrid(als.maxIter, [2,5,10]).build()\ntvs = CustomTrainValidationSplit(estimator=als,\n                           estimatorParamMaps=paramGrid,\n                           evaluator=TestEvaluator(),\n                           validation=test,\n                           training=training)\nresult = tvs.fit(training)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["prediction = result.bestModel.transform(test)\nevaluator.evaluate(prediction)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["user_recs = result.bestModel.recommendForAllUsers(3)"],"metadata":{},"outputs":[],"execution_count":38}],"metadata":{"name":"MusicBox","notebookId":832804189762642},"nbformat":4,"nbformat_minor":0}
